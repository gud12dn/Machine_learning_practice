# Machine_learning_practice


This repository contains a collection of machine learning algorithm implementations and experiments that I practiced as part of my personal learning journey in data science and machine learning.  
The goal of this project was to gain **hands-on experience** with fundamental machine learning models, understand their inner workings, and apply them to various datasets.

The practice covers both **regression** and **classification** models and includes comparison of model performance, parameter tuning, and insights on practical use cases.

---

## Motivation

As part of building a strong foundation in machine learning, I wanted to go beyond using "out-of-the-box" models and instead:
- Learn how different algorithms actually work internally.
- Understand model assumptions and when to apply each method.
- Practice model training, evaluation, and optimization workflows.
- Compare models on different kinds of problems (linear vs. non-linear, simple vs. complex).

---

## Project Structure

| Algorithm                | Notebook                        | Description |
|--------------------------|---------------------------------|-------------|
| Linear Regression        | `linear_regression.ipynb`        | Simple linear regression to predict continuous variables; model fitting, residual analysis, evaluation metrics (MSE, RÂ²). |
| Logistic Regression      | `Logistic Regression.ipynb`      | Binary classification using logistic regression; application on real-world classification datasets, ROC curve analysis. |
| Decision Tree            | `decision_tree.ipynb`            | Decision tree classifier and regressor; visualization of decision boundaries, pruning techniques, overfitting vs generalization. |
| Random Forest            | `random_forest.ipynb`            | Ensemble learning with random forests; feature importance, model stability, and performance comparison with single trees. |
| K-Nearest Neighbors (KNN)| `K_nearest_neighbors.ipynb`      | KNN classifier; exploring impact of K parameter, distance metrics, and decision boundaries. |
| XGBoost                  | `XGBoost.ipynb`                  | Advanced gradient boosting algorithm; hyperparameter tuning, regularization, and handling imbalanced data. |

---

## Technologies Used

- **Programming Language**: Python
- **Development Environment**: Jupyter Notebook
- **Libraries**:
  - `numpy`, `pandas` for data manipulation
  - `matplotlib`, `seaborn` for data visualization
  - `scikit-learn` for machine learning models and tools
  - `xgboost` for gradient boosting models

---

## Learning Outcomes

Through these practical exercises, I achieved the following:
- Gained a clear understanding of how core ML algorithms operate (mathematics + code).
- Developed skills to preprocess data and handle practical data issues.
- Learned how to train, validate, and tune models using cross-validation and grid search.
- Explored interpretability tools such as feature importance and decision boundaries.
- Practiced comparing different models and selecting the best one for a problem.
- Improved model performance awareness by exploring trade-offs (bias-variance, overfitting/underfitting).


